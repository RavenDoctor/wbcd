**Project:** Wisconsin Breast Cancer Dataset (WBCD) Testing
This project has been led by Adam Shaddick BSc Software Engineering
Collaboration with Callum Howells and Ieuan Bell-Langford 
Supervised by Dr Simon Thorne under the Concepts in Artificial Intellignce Module (Cardiff Metropolitan University)

**Development Code Log**

---

### Sprint 1 **Initial Setup**
- **Date:** [12/02/2024]
- **Task:** Set up the development environment.
- **Status:** Completed
- **Actions Taken:**
  - Installed necessary libraries: `numpy`, `pandas`, `sklearn`, `tensorflow`, `matplotlib`, `seaborn`.
  - Handled missing TensorFlow installation with a try-except block.
- **Challenges & Notes:**
  - Ensured dataset availability at `C:\Users\adams\Documents\wbcd.csv` when testing on personal work environment.
  - Checked dependency versions for compatibility.

//Note: Ensure to replace existing dataset with preprocessed version in repository - Adam

---

### Sprint 2 **Data Preprocessing**
- **Date:** [23/02/2024]
- **Task:** Data loading and preprocessing.
- **Actions Taken:**
  - Loaded dataset using `pandas.read_csv()`.
  - Separated features (`X`) and target variable (`y`).
  - Encoded categorical target variable using `LabelEncoder`.
  - Normalized feature data using `StandardScaler`.
  - Split dataset into training and test sets (80/20 split).
      //Note: This split was decided via previous weka testing installments, available upon request.
- **Challenges & Notes:**
  - Verified data integrity before proceeding.
  - Ensured class labels are correctly mapped.

---

### Sprint 3 **Model Development**
- **Date:** [1/03/2024]
- **Task:** Defined and compiled neural network model.
- **Actions Taken:**
  - Used `Sequential()` model with three layers:
    - Input layer: 128 neurons, ReLU activation.
    - Hidden layer: 64 neurons, ReLU activation.
    - Output layer: Softmax activation for multi-class classification.
  - Compiled model with `adam` optimizer and `sparse_categorical_crossentropy` loss.
- **Challenges & Notes:**
  - Tuned architecture for optimal performance.
  - Considered other activation functions but chose ReLU for efficiency.

---

### Sprint 4 **Model Training**
- **Date:** [5/03/2024]
- **Task:** Trained the model on the dataset.
- **Actions Taken:**
  - Trained the model for 20 epochs with a batch size of 32.
  - Used `validation_split=0.1` to monitor performance.
- **Challenges & Notes:**
  - Monitored accuracy trends over epochs.
  - Prevented overfitting with validation data.

---

### Sprint 5 **Model Evaluation**
- **Date:** [8/03/2024]
- **Task:** Evaluated the model's performance.
- **Actions Taken:**
  - Evaluated test accuracy using `model.evaluate()`.
  - Generated predictions and calculated confusion matrix.
- **Challenges & Notes:**
  - Verified that test accuracy is within an acceptable range.
  - Addressed any class imbalance issues.

---

###  Sprint 6 **Visualization & Analysis**
- **Date:** [14/03/2024]
- **Task:** Generated model performance visualizations.
- **Actions Taken:**
  - Plotted confusion matrix using `seaborn.heatmap()`.
  - Plotted learning curve (accuracy vs. epochs).
- **Challenges & Notes:**
  - Interpreted confusion matrix for insights.
  - Assessed learning curve for potential improvements.

---

### Sprint 7 **Next Steps**
- Awaiting further installments
- Tune hyperparameters (learning rate, epochs, batch size).
- Experiment with additional hidden layers.
- Implement early stopping to prevent overfitting.
- Deploy model using Flask/Django for real-time inference.
  - Use existing web development skills, hand off to Ieuan

**End of Log**

